
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>seldonian.optimizers.gradient_descent.gradient_descent_adam &#8212; Seldonian Engine pre-release documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="seldonian.optimizers.gradient_descent.setup_gradients" href="seldonian.optimizers.gradient_descent.setup_gradients.html" />
    <link rel="prev" title="seldonian.optimizers.gradient_descent" href="seldonian.optimizers.gradient_descent.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="seldonian-optimizers-gradient-descent-gradient-descent-adam">
<h1>seldonian.optimizers.gradient_descent.gradient_descent_adam<a class="headerlink" href="#seldonian-optimizers-gradient-descent-gradient-descent-adam" title="Permalink to this heading">¶</a></h1>
<dl class="py function">
<dt class="sig sig-object py" id="seldonian.optimizers.gradient_descent.gradient_descent_adam">
<span class="sig-name descname"><span class="pre">gradient_descent_adam</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">primary_objective</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">upper_bounds_function</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">theta_init</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_init</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha_theta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha_lamb</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta_velocity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta_rmsprop</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_iters</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_library</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'autograd'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">store_values</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">debug</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#seldonian.optimizers.gradient_descent.gradient_descent_adam" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements simultaneous gradient descent/ascent using 
the “adam” optimizer on a Lagrangian:
L(theta,lambda) = f(theta) + lambda*g(theta),
where f is the primary objective, lambda is a vector of 
Lagrange multipliers, and g is a vector of the 
upper bound functions. Gradient descent is done for theta 
and gradient ascent is done for lambda to find the saddle 
points of L.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>primary_objective</strong> (<em>function</em><em> or </em><em>class method</em>) – The objective function that would
be solely optimized in the absence of behavioral constraints,
i.e. the loss function</p></li>
<li><p><strong>upper_bounds_function</strong> (<em>function</em><em> or </em><em>class method</em>) – The function that calculates
the upper bounds on the constraints</p></li>
<li><p><strong>theta_init</strong> (<em>float</em>) – Initial model weights</p></li>
<li><p><strong>lambda_init</strong> – Initial values for Lagrange multiplier terms</p></li>
<li><p><strong>alpha_theta</strong> (<em>float</em>) – Initial learning rate for theta</p></li>
<li><p><strong>alpha_lamb</strong> (<em>float</em>) – Learning rate for lambda</p></li>
<li><p><strong>beta_velocity</strong> (<em>float</em>) – Exponential decay rate for velocity term</p></li>
<li><p><strong>beta_rmsprop</strong> (<em>float</em>) – Exponential decay rate for rmsprop term</p></li>
<li><p><strong>num_iters</strong> (<em>int</em>) – The number of iterations of gradient descent to run</p></li>
<li><p><strong>gradient_library</strong> (<em>str</em><em>, </em><em>defaults to &quot;autograd&quot;</em>) – The name of the library to use for computing 
automatic gradients.</p></li>
<li><p><strong>store_values</strong> (<em>bool</em>) – Whether to include evaluations of various 
quantities in the solution dictionary</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>solution, a dictionary containing the solution and metadata 
about the gradient descent run, if store_values==True</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">Seldonian Engine</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Getting Started</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../api.html">API</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="seldonian.html">seldonian</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="seldonian.RL.html">seldonian.RL</a></li>
<li class="toctree-l3"><a class="reference internal" href="seldonian.candidate_selection.html">seldonian.candidate_selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="seldonian.dataset.html">seldonian.dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="seldonian.models.html">seldonian.models</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="seldonian.optimizers.html">seldonian.optimizers</a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="seldonian.optimizers.gradient_descent.html">seldonian.optimizers.gradient_descent</a><ul class="current">
<li class="toctree-l5 current"><a class="current reference internal" href="#">seldonian.optimizers.gradient_descent.gradient_descent_adam</a></li>
<li class="toctree-l5"><a class="reference internal" href="seldonian.optimizers.gradient_descent.setup_gradients.html">seldonian.optimizers.gradient_descent.setup_gradients</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="seldonian.parse_tree.html">seldonian.parse_tree</a></li>
<li class="toctree-l3"><a class="reference internal" href="seldonian.safety_test.html">seldonian.safety_test</a></li>
<li class="toctree-l3"><a class="reference internal" href="seldonian.seldonian_algorithm.html">seldonian.seldonian_algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="seldonian.spec.html">seldonian.spec</a></li>
<li class="toctree-l3"><a class="reference internal" href="seldonian.utils.html">seldonian.utils</a></li>
<li class="toctree-l3"><a class="reference internal" href="seldonian.warnings.html">seldonian.warnings</a></li>
</ul>
</li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  <li><a href="../api.html">API</a><ul>
  <li><a href="seldonian.html">seldonian</a><ul>
  <li><a href="seldonian.optimizers.html">seldonian.optimizers</a><ul>
  <li><a href="seldonian.optimizers.gradient_descent.html">seldonian.optimizers.gradient_descent</a><ul>
      <li>Previous: <a href="seldonian.optimizers.gradient_descent.html" title="previous chapter">seldonian.optimizers.gradient_descent</a></li>
      <li>Next: <a href="seldonian.optimizers.gradient_descent.setup_gradients.html" title="next chapter">seldonian.optimizers.gradient_descent.setup_gradients</a></li>
  </ul></li>
  </ul></li>
  </ul></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2022, University of Massachusetts.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 5.1.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/_autosummary/seldonian.optimizers.gradient_descent.gradient_descent_adam.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>